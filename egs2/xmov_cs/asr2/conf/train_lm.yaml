lm: transformer
lm_conf:
    pos_enc: null
    embed_unit: 128
    att_unit: 512
    head: 8
    unit: 2048
    layer: 16
    dropout_rate: 0.1

num_workers: 1
# optimization related
grad_clip: 5.0
#batch_type: folded
#batch_size: 400   # batch size in LM training
batch_type: numel
batch_bins: 2000000000  # for 4 A-100
accum_grad: 4
max_epoch: 100
patience: 20

num_iters_per_epoch: 64000  # save ckpt every N steps, for 128 split, each split 500 steps

optim: adam
optim_conf:
   lr: 0.001
scheduler: warmuplr
scheduler_conf:
   warmup_steps: 25000

best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 10  # 10 is good.
